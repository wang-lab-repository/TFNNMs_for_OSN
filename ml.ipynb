{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# import libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use(\"default\")\n",
    "from sklearn.metrics import mean_squared_error, r2_score,mean_absolute_percentage_error,mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.multioutput import RegressorChain\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import LinearRegression,Lasso,Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from math import sqrt\n",
    "from sklearn import linear_model\n",
    "from sklearn.neighbors import NearestCentroid,KNeighborsRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from utils import feature_impute, feature_impute_exiting, rf_fill, select_feature, plot_SSP\n",
    "import random\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "import time\n",
    "import datetime"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df_all=pd.read_excel(\"data.xlsx\")\n",
    "unseen=pd.read_excel(\"unseen_data_fingernet.xlsx\")\n",
    "# df_all.columns\n",
    "Yc=['RP','RR']\n",
    "y=df_all[Yc]\n",
    "x=df_all.drop(Yc,axis=1)\n",
    "y_unseen = unseen[Yc]\n",
    "x_unseen = unseen.drop(Yc,axis=1)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-10T12:02:53.928323Z",
     "iopub.execute_input": "2023-07-10T12:02:53.928680Z",
     "iopub.status.idle": "2023-07-10T12:02:54.430385Z",
     "shell.execute_reply.started": "2023-07-10T12:02:53.928650Z",
     "shell.execute_reply": "2023-07-10T12:02:54.428509Z"
    },
    "trusted": true
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "need_delete=['Surface roughness']\n",
    "need_statistic_filling=['Permeability','Rejection/Selectivity','Amine concentration','Chloride concentration']\n",
    "need_model_filling=['Water contact angle','Solute concentration','NP size','Solute molecular weight']\n",
    "needed_model_filling=['Amine concentration','Chloride concentration','Chloride concentration','Rejection/Selectivity']\n",
    "x.drop(need_delete,axis=1,inplace=True)\n",
    "x_unseen.drop(need_delete,axis=1,inplace=True)\n",
    "filling=x[need_statistic_filling].mean()\n",
    "x[need_statistic_filling]=x[need_statistic_filling].fillna(dict(filling), inplace=False) ##\n",
    "x_unseen[need_statistic_filling]=x_unseen[need_statistic_filling].fillna(dict(filling), inplace=False) ##"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-10T12:02:56.559085Z",
     "iopub.execute_input": "2023-07-10T12:02:56.559713Z",
     "iopub.status.idle": "2023-07-10T12:02:56.577709Z",
     "shell.execute_reply.started": "2023-07-10T12:02:56.559680Z",
     "shell.execute_reply": "2023-07-10T12:02:56.576580Z"
    },
    "trusted": true
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model=[]\n",
    "for target, use_f in zip(need_model_filling,needed_model_filling):\n",
    "    x, m=feature_impute(x, target, use_f)\n",
    "    model.append(m)\n",
    "    rf_fill(x, target)\n",
    "for m, target, use_f in zip(model, need_model_filling, needed_model_filling):\n",
    "    x_unseen=feature_impute_exiting(x_unseen, target, use_f, model=m)\n",
    "    rf_fill(x_unseen, target)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-10T12:02:57.402284Z",
     "iopub.execute_input": "2023-07-10T12:02:57.405010Z",
     "iopub.status.idle": "2023-07-10T12:02:59.522734Z",
     "shell.execute_reply.started": "2023-07-10T12:02:57.404964Z",
     "shell.execute_reply": "2023-07-10T12:02:59.520906Z"
    },
    "trusted": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df_all = pd.concat([x,x_unseen],axis=0)\n",
    "col = ['Logp', 'Tpsa', 'Qed', 'Hba', 'Hbd', 'Rob']\n",
    "col1 = ['Solvent_logp', 'Solvent_tpsa', 'Solvent_qed', 'Solvent_hba', 'Solvent_hbd', 'Solvent_rob']\n",
    "col2 = ['Solute_logp', 'Solute_tpsa', 'Solute_qed', 'Solute_hba', 'Solute_hbd', 'Solute_rob']\n",
    "smiles=pd.read_excel(\"smiles.xlsx\")\n",
    "solute = []\n",
    "for i in range(df_all.shape[0]):\n",
    "    if df_all.iloc[i]['Solute_type'] == 'AO':\n",
    "        solute.append(smiles.loc[10][col])\n",
    "    elif df_all.iloc[i]['Solute_type'] == 'BTB':\n",
    "        solute.append(smiles.loc[11][col])\n",
    "    elif df_all.iloc[i]['Solute_type'] == 'CV':\n",
    "        solute.append(smiles.loc[12][col])\n",
    "    elif df_all.iloc[i]['Solute_type'] == 'MB':\n",
    "        solute.append(smiles.loc[13][col])\n",
    "    elif df_all.iloc[i]['Solute_type'] == 'MO':\n",
    "        solute.append(smiles.loc[14][col])\n",
    "    elif df_all.iloc[i]['Solute_type'] == 'PEG':\n",
    "        solute.append(smiles.loc[15][col])\n",
    "    elif df_all.iloc[i]['Solute_type'] == 'RB':\n",
    "        solute.append(smiles.loc[16][col])\n",
    "    elif df_all.iloc[i]['Solute_type'] == 'RDB':\n",
    "        solute.append(smiles.loc[17][col])\n",
    "    elif df_all.iloc[i]['Solute_type'] == 'tetracycline':\n",
    "        solute.append(smiles.loc[18][col])\n",
    "    elif df_all.iloc[i]['Solute_type'] == 'oligomer':\n",
    "        solute.append(smiles.loc[19][col])\n",
    "    elif df_all.iloc[i]['Solute_type'] == 'SY':\n",
    "        solute.append(smiles.loc[20][col])\n",
    "    elif df_all.iloc[i]['Solute_type'] == 'EB':\n",
    "        solute.append(smiles.loc[21][col])\n",
    "    elif df_all.iloc[i]['Solute_type'] == 'RBBR':\n",
    "        solute.append(smiles.loc[22][col])\n",
    "    elif df_all.iloc[i]['Solute_type'] == 'SO':\n",
    "        solute.append(smiles.loc[23][col])\n",
    "df_all[col2] = solute"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-10T12:02:59.524708Z",
     "iopub.execute_input": "2023-07-10T12:02:59.525048Z",
     "iopub.status.idle": "2023-07-10T12:03:00.557480Z",
     "shell.execute_reply.started": "2023-07-10T12:02:59.525021Z",
     "shell.execute_reply": "2023-07-10T12:03:00.555937Z"
    },
    "trusted": true
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# For data after molecular characterization of solvents and solutes,\n",
    "# solvent type and solute type should be removed to prevent redundancy of information.\n",
    "x=df_all.copy()\n",
    "x.drop(['Solute_type'], axis=1, inplace=True) #, 'Solvent_type'\n",
    "x=pd.get_dummies(x)\n",
    "is_delete = []\n",
    "for c in col2: # +col1\n",
    "    if x[0:653][c].max() == 0:\n",
    "        is_delete.append(c)\n",
    "x.drop(columns=is_delete, axis=1, inplace=True)\n",
    "# Use the minimum and maximum values of the former\n",
    "min = x.iloc[:653,:].min()\n",
    "max = x.iloc[:653,:].max()\n",
    "# Make changes to the entire dataset\n",
    "x = (x - min) / max\n",
    "x_unseen=x.iloc[653:, :]\n",
    "x=x.iloc[:653, :]\n",
    "# x, x_unseen = select_feature(x, y, x_unseen)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, shuffle = True, random_state = 60)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-10T12:03:00.559199Z",
     "iopub.execute_input": "2023-07-10T12:03:00.559769Z",
     "iopub.status.idle": "2023-07-10T12:03:03.460520Z",
     "shell.execute_reply.started": "2023-07-10T12:03:00.559735Z",
     "shell.execute_reply": "2023-07-10T12:03:03.459757Z"
    },
    "trusted": true
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tree_model=[\n",
    "    RandomForestRegressor(n_estimators=100, max_leaf_nodes=10, max_features=100, max_depth=15, random_state=42),\n",
    "    XGBRegressor(subsample=0.7,reg_lambda=1,reg_alpha=0.01,n_estimators=1000,min_child_weight=1,max_depth=15,\n",
    "    learning_rate=0.05,gamma=0.1,colsample_bytree=0.7,random_state=42),\n",
    "    AdaBoostRegressor(n_estimators=1000,learning_rate=1.3,random_state=42),\n",
    "    GradientBoostingRegressor(learning_rate=0.01,n_estimators=100,subsample=0.5,random_state=42, max_features=100),\n",
    "    LGBMRegressor(max_depth = 15,learning_rate=0.03, n_estimators=100, seed = 42),\n",
    "    DecisionTreeRegressor(max_leaf_nodes=10, max_features=100, max_depth=15, random_state=42),\n",
    "    BaggingRegressor(n_estimators=100,max_samples=0.70, max_features=0.75, bootstrap=True, random_state=42)\n",
    "            ]\n",
    "\n",
    "untree_model=[\n",
    "    Lasso(alpha=0.0),\n",
    "    Ridge(),\n",
    "    linear_model.ElasticNet(alpha=0.1, l1_ratio=0.1),\n",
    "    linear_model.BayesianRidge(n_iter=500, tol=0.006, alpha_1=3e-06, alpha_2=3e-06, lambda_1=2e-06, lambda_2=2e-06),\n",
    "    KNeighborsRegressor(n_neighbors=4),\n",
    "    SVR(kernel='linear')]\n",
    "\n",
    "model_name=['RF','XGB','AdaB','GradB','LGBM','DT','Bag','Lasso','Ridge','ENet','Bayes','KNN','SVM']"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-10T12:03:03.949334Z",
     "iopub.execute_input": "2023-07-10T12:03:03.949732Z",
     "iopub.status.idle": "2023-07-10T12:03:03.959527Z",
     "shell.execute_reply.started": "2023-07-10T12:03:03.949687Z",
     "shell.execute_reply": "2023-07-10T12:03:03.958255Z"
    },
    "trusted": true
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "kfold = KFold(n_splits = 5, shuffle=True, random_state=6)\n",
    "results=[]\n",
    "def get_ml_multioutput_predict(m):\n",
    "    scores1 = []\n",
    "    scores2 = []\n",
    "    t_scores1 = []\n",
    "    t_scores2 = []\n",
    "\n",
    "    rmse1 = []\n",
    "    rmse2 = []\n",
    "    t_rmse1 = []\n",
    "    t_rmse2 = []\n",
    "\n",
    "    mape1 = []\n",
    "    mape2 = []\n",
    "    t_mape1 = []\n",
    "    t_mape2 = []\n",
    "\n",
    "    mae1 = []\n",
    "    mae2 = []\n",
    "    t_mae1 = []\n",
    "    t_mae2 = []\n",
    "    for train, test in kfold.split(x_train, y_train):\n",
    "        model = MultiOutputRegressor(m)\n",
    "        model.fit(x_train.iloc[train], y_train.iloc[train])\n",
    "        y_train_pred=model.predict(x_train.iloc[train])\n",
    "        y_val_pred=model.predict(x_train.iloc[test])\n",
    "        scores1.append(r2_score(y_train.iloc[test].values[:,0],y_val_pred[:,0]))\n",
    "        scores2.append(r2_score(y_train.iloc[test].values[:,1],y_val_pred[:,1]))\n",
    "        t_scores1.append(r2_score(y_train.iloc[train].values[:,0],y_train_pred[:,0]))\n",
    "        t_scores2.append(r2_score(y_train.iloc[train].values[:,1],y_train_pred[:,1]))\n",
    "        rmse1.append(sqrt(mean_squared_error(y_train.iloc[test].values[:,0],y_val_pred[:,0])))\n",
    "        rmse2.append(sqrt(mean_squared_error(y_train.iloc[test].values[:,1],y_val_pred[:,1])))\n",
    "        t_rmse1.append(sqrt(mean_squared_error(y_train.iloc[train].values[:,0],y_train_pred[:,0])))\n",
    "        t_rmse2.append(sqrt(mean_squared_error(y_train.iloc[train].values[:,1],y_train_pred[:,1])))\n",
    "        mape1.append(mean_absolute_percentage_error(y_train.iloc[test].values[:,0],y_val_pred[:,0]))\n",
    "        mape2.append(mean_absolute_percentage_error(y_train.iloc[test].values[:,1],y_val_pred[:,1]))\n",
    "        t_mape1.append(mean_absolute_percentage_error(y_train.iloc[train].values[:,0],y_train_pred[:,0]))\n",
    "        t_mape2.append(mean_absolute_percentage_error(y_train.iloc[train].values[:,1],y_train_pred[:,1]))\n",
    "        mae1.append(mean_absolute_error(y_train.iloc[test].values[:,0],y_val_pred[:,0]))\n",
    "        mae2.append(mean_absolute_error(y_train.iloc[test].values[:,1],y_val_pred[:,1]))\n",
    "        t_mae1.append(mean_absolute_error(y_train.iloc[train].values[:,0],y_train_pred[:,0]))\n",
    "        t_mae2.append(mean_absolute_error(y_train.iloc[train].values[:,1],y_train_pred[:,1]))\n",
    "    result=[]\n",
    "    result.append(np.array(scores1).mean())\n",
    "    result.append(np.array(scores1).std())\n",
    "    result.append(np.array(scores2).mean())\n",
    "    result.append(np.array(scores2).std())\n",
    "    result.append(np.array(rmse1).mean())\n",
    "    result.append(np.array(rmse1).std())\n",
    "    result.append(np.array(rmse2).mean())\n",
    "    result.append(np.array(rmse2).std())\n",
    "    result.append(np.array(mape1).mean())\n",
    "    result.append(np.array(mape1).std())\n",
    "    result.append(np.array(mape2).mean())\n",
    "    result.append(np.array(mape2).std())\n",
    "    result.append(np.array(mae1).mean())\n",
    "    result.append(np.array(mae1).std())\n",
    "    result.append(np.array(mae2).mean())\n",
    "    result.append(np.array(mae2).std())\n",
    "    result.append(np.array(t_scores1).mean())\n",
    "    result.append(np.array(t_scores1).std())\n",
    "    result.append(np.array(t_scores2).mean())\n",
    "    result.append(np.array(t_scores2).std())\n",
    "    result.append(np.array(t_rmse1).mean())\n",
    "    result.append(np.array(t_rmse1).std())\n",
    "    result.append(np.array(t_rmse2).mean())\n",
    "    result.append(np.array(t_rmse2).std())\n",
    "    result.append(np.array(t_mape1).mean())\n",
    "    result.append(np.array(t_mape1).std())\n",
    "    result.append(np.array(t_mape2).mean())\n",
    "    result.append(np.array(t_mape2).std())\n",
    "    result.append(np.array(t_mae1).mean())\n",
    "    result.append(np.array(t_mae1).std())\n",
    "    result.append(np.array(t_mae2).mean())\n",
    "    result.append(np.array(t_mae2).std())\n",
    "    results.append(result)\n",
    "    print('r2:')\n",
    "    print(f'train_mean1:{np.array(t_scores1).mean()}, train_std1:{np.array(t_scores1).std()}, \\\n",
    "            train_mean2:{np.array(t_scores2).mean()},train_std2:{np.array(t_scores2).std()},\\\n",
    "            test_mean1:{np.array(scores1).mean()}, test_std1:{np.array(scores1).std()},\\\n",
    "            test_mean2:{np.array(scores2).mean()}, test_std2:{np.array(scores2).std()}')\n",
    "    print('\\n')\n",
    "    print('rmse:')\n",
    "    print(f'train_mean1:{np.array(t_rmse1).mean()}, train_std1:{np.array(t_rmse1).std()}, \\\n",
    "            train_mean2:{np.array(t_rmse2).mean()},train_std2:{np.array(t_rmse2).std()},\\\n",
    "            test_mean1:{np.array(rmse1).mean()}, test_std1:{np.array(rmse1).std()},\\\n",
    "            test_mean2:{np.array(rmse2).mean()}, test_std2:{np.array(rmse2).std()}')\n",
    "    print('\\n')\n",
    "    print('mape:')\n",
    "    print(f'train_mean1:{np.array(t_mape1).mean()}, train_std1:{np.array(t_mape1).std()}, \\\n",
    "            train_mean2:{np.array(t_mape2).mean()},train_std2:{np.array(t_mape2).std()},\\\n",
    "            test_mean1:{np.array(mape1).mean()}, test_std1:{np.array(mape1).std()},\\\n",
    "            test_mean2:{np.array(mape2).mean()}, test_std2:{np.array(mape2).std()}')\n",
    "    print('\\n')\n",
    "    print('mae:')\n",
    "    print(f'train_mean1:{np.array(t_mae1).mean()}, train_std1:{np.array(t_mae1).std()}, \\\n",
    "            train_mean2:{np.array(t_mae2).mean()},train_std2:{np.array(t_mae2).std()},\\\n",
    "            test_mean1:{np.array(mae1).mean()}, test_std1:{np.array(mae1).std()},\\\n",
    "            test_mean2:{np.array(mae2).mean()}, test_std2:{np.array(mae2).std()}')"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for m in tree_model+untree_model:\n",
    "    print(m)\n",
    "    print('\\n')\n",
    "    print(get_ml_multioutput_predict(m))\n",
    "# pd.DataFrame(np.array(results)).to_excel('ml_cross_validation.xlsx')"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train(m, seed=6):\n",
    "    i=0\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    try:\n",
    "        m.set_params(random_state=seed)\n",
    "    except:\n",
    "        i=i+1\n",
    "    try:\n",
    "        m.set_params(seed=seed)\n",
    "    except:\n",
    "        i=i+1\n",
    "    model = MultiOutputRegressor(m)\n",
    "    model.fit(x_train, y_train)\n",
    "    y_predt = model.predict(x_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_predu = model.predict(x_unseen)\n",
    "#     print(y_predu)\n",
    "    r2_test_rwp.append(r2_score(y_test.values[:, 0],y_pred[:, 0]))\n",
    "    r2_test_rsp.append(r2_score(y_test.values[:, 1],y_pred[:, 1]))\n",
    "    rmse_test_rwp.append(sqrt(mean_squared_error(y_test.values[:,0],y_pred[:,0])))\n",
    "    rmse_test_rsp.append(sqrt(mean_squared_error(y_test.values[:,1],y_pred[:,1])))\n",
    "    mape_test_rwp.append(mean_absolute_percentage_error(y_test.values[:,0],y_pred[:,0]))\n",
    "    mape_test_rsp.append(mean_absolute_percentage_error(y_test.values[:,1],y_pred[:,1]))\n",
    "    mae_test_rwp.append(mean_absolute_error(y_test.values[:,0],y_pred[:,0]))\n",
    "    mae_test_rsp.append(mean_absolute_error(y_test.values[:,1],y_pred[:,1]))\n",
    "    \n",
    "    r2_train_rwp.append(r2_score(y_train.values[:, 0],y_predt[:, 0]))\n",
    "    r2_train_rsp.append(r2_score(y_train.values[:, 1],y_predt[:, 1]))\n",
    "    rmse_train_rwp.append(sqrt(mean_squared_error(y_train.values[:, 0],y_predt[:, 0])))\n",
    "    rmse_train_rsp.append(sqrt(mean_squared_error(y_train.values[:, 1],y_predt[:, 1])))\n",
    "    mape_train_rwp.append(mean_absolute_percentage_error(y_train.values[:, 0],y_predt[:, 0]))\n",
    "    mape_train_rsp.append(mean_absolute_percentage_error(y_train.values[:, 1],y_predt[:, 1]))\n",
    "    mae_train_rwp.append(mean_absolute_error(y_train.values[:, 0],y_predt[:, 0]))\n",
    "    mae_train_rsp.append(mean_absolute_error(y_train.values[:, 1],y_predt[:, 1]))\n",
    "    print('R2:',r2_score(y_test.values[:, 0],y_pred[:, 0]), r2_score(y_test.values[:, 1],y_pred[:, 1]))\n",
    "    print(\"rmse:\",sqrt(mean_squared_error(y_test.values[:,0],y_pred[:,0])), sqrt(mean_squared_error(y_test.values[:,1],y_pred[:,1])))\n",
    "    print(\"mape:\",mean_absolute_percentage_error(y_test.values[:,0],y_pred[:,0]), mean_absolute_percentage_error(y_test.values[:,1],y_pred[:,1]))\n",
    "    print(\"mae:\",mean_absolute_error(y_test.values[:,0],y_pred[:,0]), mean_absolute_error(y_test.values[:,1],y_pred[:,1]))\n",
    "    return y_predu"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-10T12:05:06.509867Z",
     "iopub.execute_input": "2023-07-10T12:05:06.510248Z",
     "iopub.status.idle": "2023-07-10T12:05:06.525999Z",
     "shell.execute_reply.started": "2023-07-10T12:05:06.510217Z",
     "shell.execute_reply": "2023-07-10T12:05:06.524797Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import time\n",
    "import datetime\n",
    "import random\n",
    "starttime = datetime.datetime.now()\n",
    "results=[]\n",
    "for m_name, m in zip(model_name, tree_model+untree_model):\n",
    "    result=[]\n",
    "    print(m_name)\n",
    "    r2_test_rwp=[]\n",
    "    r2_test_rsp=[]\n",
    "    rmse_test_rwp=[]\n",
    "    rmse_test_rsp=[]\n",
    "    mape_test_rwp=[]\n",
    "    mape_test_rsp=[]\n",
    "    mae_test_rwp=[]\n",
    "    mae_test_rsp=[]\n",
    "    r2_train_rwp=[]\n",
    "    r2_train_rsp=[]\n",
    "    rmse_train_rwp=[]\n",
    "    rmse_train_rsp=[]\n",
    "    mape_train_rwp=[]\n",
    "    mape_train_rsp=[]\n",
    "    mae_train_rwp=[]\n",
    "    mae_train_rsp=[]\n",
    "    y_unseen_sum=np.zeros(y_unseen.shape)\n",
    "    for n, i in enumerate([random.choice([i for i in range(1000)]) for i in range(100)]):\n",
    "        y_unseen_temp = train(m, seed=i)\n",
    "        y_unseen_sum = y_unseen_sum+y_unseen_temp\n",
    "        endtime = datetime.datetime.now()\n",
    "        print(m_name+f'_随机种子测量完成，目前已耗时{(endtime - starttime).seconds} s')\n",
    "    for i in [\n",
    "            r2_test_rwp,r2_test_rsp,rmse_test_rwp,rmse_test_rsp,mape_test_rwp,mape_test_rsp,mae_test_rwp,mae_test_rsp, \n",
    "            r2_train_rwp,r2_train_rsp,rmse_train_rwp,rmse_train_rsp,mape_train_rwp,mape_train_rsp,mae_train_rwp,mae_train_rsp\n",
    "                ]:\n",
    "        result.append(np.array(i).mean())\n",
    "        result.append(np.array(i).std())\n",
    "    results.append(result)\n",
    "    r2_train_rwp,r2_train_rsp,rmse_train_rwp,rmse_train_rsp,mape_train_rwp,mape_train_rsp,\\\n",
    "    mae_train_rwp,mae_train_rsp,r2_test_rwp,r2_test_rsp,rmse_test_rwp,rmse_test_rsp,mape_test_rwp,\\\n",
    "    mape_test_rsp,mae_test_rwp,mae_test_rsp = pd.DataFrame(r2_train_rwp),pd.DataFrame(r2_train_rsp),\\\n",
    "    pd.DataFrame(rmse_train_rwp),pd.DataFrame(rmse_train_rsp),pd.DataFrame(mape_train_rwp),\\\n",
    "    pd.DataFrame(mape_train_rsp),pd.DataFrame(mae_train_rwp),pd.DataFrame(mae_train_rsp),\\\n",
    "    pd.DataFrame(r2_test_rwp),pd.DataFrame(r2_test_rsp),pd.DataFrame(rmse_test_rwp),\\\n",
    "    pd.DataFrame(rmse_test_rsp),pd.DataFrame(mape_test_rwp),pd.DataFrame(mape_test_rsp),\\\n",
    "    pd.DataFrame(mae_test_rwp),pd.DataFrame(mae_test_rsp)\n",
    "    ml_metrics=pd.concat([r2_train_rwp,r2_train_rsp,rmse_train_rwp,rmse_train_rsp,mape_train_rwp,mape_train_rsp,mae_train_rwp,mae_train_rsp,\n",
    "            r2_test_rwp,r2_test_rsp,rmse_test_rwp,rmse_test_rsp,mape_test_rwp,mape_test_rsp,mae_test_rwp,mae_test_rsp],axis=1)\n",
    "    ml_metrics.to_excel(m_name+'_metrics.xlsx')\n",
    "    y_unseen_sum_last=y_unseen_sum.copy()\n",
    "    y_unseen_sum_last=y_unseen_sum_last/100\n",
    "#     print('R2:',r2_score(y_unseen.values[:, 0],y_unseen_sum_last[:, 0]), r2_score(y_unseen.values[:, 1],y_unseen_sum_last[:, 1]))\n",
    "#     print(\"rmse:\",sqrt(mean_squared_error(y_unseen.values[:,0],y_unseen_sum_last[:,0])), sqrt(mean_squared_error(y_unseen.values[:,1],y_unseen_sum_last[:,1])))\n",
    "#     print(\"mape:\",mean_absolute_percentage_error(y_unseen.values[:,0],y_unseen_sum_last[:,0]), mean_absolute_percentage_error(y_unseen.values[:,1],y_unseen_sum_last[:,1]))\n",
    "#     print(\"mae:\",mean_absolute_error(y_unseen.values[:,0],y_unseen_sum_last[:,0]), mean_absolute_error(y_unseen.values[:,1],y_unseen_sum_last[:,1]))\n",
    "    pd.DataFrame(y_unseen_sum_last).to_excel(m_name+'_unseen.xlsx')\n",
    "endtime = datetime.datetime.now()\n",
    "print(f'The total time taken for this repeat measurement：{(endtime - starttime).seconds}')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-10T12:05:19.639286Z",
     "iopub.execute_input": "2023-07-10T12:05:19.639654Z",
     "iopub.status.idle": "2023-07-10T12:05:25.136616Z",
     "shell.execute_reply.started": "2023-07-10T12:05:19.639623Z",
     "shell.execute_reply": "2023-07-10T12:05:25.135665Z"
    },
    "trusted": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# np.array(results).shape\n",
    "# pd.DataFrame(np.array(results)).to_excel('ml_performance.xlsx')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-10T12:10:28.552591Z",
     "iopub.execute_input": "2023-07-10T12:10:28.553285Z",
     "iopub.status.idle": "2023-07-10T12:10:28.571802Z",
     "shell.execute_reply.started": "2023-07-10T12:10:28.553254Z",
     "shell.execute_reply": "2023-07-10T12:10:28.571126Z"
    },
    "trusted": true
   },
   "execution_count": 14,
   "outputs": []
  }
 ]
}
